<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge">
    <meta name="keywords" content="asyncvla, navigation, robotics, foundation model">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYH3N96LN5"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-ZYH3N96LN5');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/slick.css">
    <link rel="stylesheet" href="./static/css/slick-theme.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="static/css/custom.css">    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/slick.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://svl.stanford.edu/projects/dvmpc/">
                            DVMPC: Deep Visual MPC-Policy Learning for Navigation
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/exaug-nav">
                            ExAug: Robot-conditioned Navigation Policies via Geometric Experience Augmentation
                        </a>                      
                        <a class="navbar-item" href="https://general-navigation-models.github.io/vint/index.html">
                            ViNT: A Foundation Model for Visual Navigation
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/sacson-review/home">
                            SACSoN: Scalable Autonomous Control for Social Navigation
                        </a>
                        <a class="navbar-item" href="https://learning-language-navigation.github.io/">
                            LeLaN: Learning A Language-conditioned Navigation Policy from In-the-Wild Video
                        </a>                          
                        <a class="navbar-item" href="https://model-base-reannotation.github.io/">
                            Learning to Drive Anyware with Model-Based Reannotation
                        </a>     
                        <a class="navbar-item" href="https://omnivla-nav.github.io/">
                            OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation
                        </a>                                                  
                    </div>
                </div>
            </div>

        </div>
    </nav>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge</h1>                        
                                       
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a><sup>1, 2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://catglossop.github.io/">Catherine Glossop</a><sup>1</sup>,
                            </span>                            
                            <span class="author-block">
                                <a href="https://robodhruv.github.io/">Dhruv Shah</a><sup>3</sup>,
                            </span>                            
                            <span class="author-block">
                                <a href="https://cs.berkeley.edu/~svlevine">Sergey Levine</a><sup>1</sup>
                            </span>                            
                        </div>

                        <div class="is-size-6 publication-authors">
                            <span class="author-block"> <sup>1</sup> University of California, Berkeley,   <sup>2</sup> Toyota Motor North America,   <sup>3</sup> Princeton University</span>
                        </div>
                        <br>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2509.19480"
                                        class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                    <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>Paper</span>
                                    </a>
                                </span>                            
                                <!-- Talk Link. -->
                                <span class="link-block">
                                    <a href="https://youtu.be/uPi5YGL4JpA"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-youtube"></i>
                                        </span>
                                        <span>YouTube</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/NHirose/AsyncVLA"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github-alt"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <!-- Checkpoint Link-->
                                <span class="link-block">
                                    <a href="https://huggingface.co/NHirose/AsyncVLA_release"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-brain"></i>
                                        </span>
                                        <span>Checkpoint</span>
                                    </a>
                                </span>
                                <!-- BibTex -->
                                <span class="link-block">
                                    <a href="./static/asyncvla.bib"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-quote-left"></i> </span>
                                        <span>BibTex</span>
                                    </a>
                                </span>

                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

<section class="section pt-4 pb-5">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">

        <!-- Key Highlights -->
        <div class="content has-text-justified mb-6">
          <h2 class="title is-4">Key Highlights</h2>
          <p>
          <strong>AsyncVLA</strong> is an asynchronous control framework that enables real-time deployment of large robotic foundation models while handling network and inference latency.
          </p>
          <ol style="display: inline-block; text-align: left; font-size: 18px; line-height: 1.6;">
            <li><strong>Asynchronous VLA:</strong> High-level reasoning is seperate from onboard reactive control.</li>
            <li><strong>End-to-End Learning:</strong> Base VLA is aligned with the edge control for rapid adaptation.</li>
            <li><strong>Robust Real-World Performance:</strong> High success with few collisions under latency.</li>
          </ol>
        </div>


        <div class="publication-video">
          <video autoplay controls muted loop playsinline width="80%">
            <source src="static/videos/tesear_video.mp4" type="video/mp4">
          </video>
        </div>

        <h2 class="title is-3 has-text-centered">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robotic foundation models generalize well via internet-scale vision-language representations, but their high computational cost causes latency that breaks the control loop, making them unsafe for real-time deployment. We propose AsyncVLA, an asynchronous control framework that decouples semantic reasoning from reactive execution. Inspired by hierarchical control, AsyncVLA runs a large foundation model on a remote workstation for high-level guidance, while a lightweight onboard Edge Adapter continuously refines actions. To align these asynchronous streams, we introduce end-to-end finetuning and a trajectory re-weighting strategy prioritizing dynamic interactions. On real-world vision-based navigation tasks with delays up to 6 seconds, AsyncVLA achieves a 40% higher success rate than state-of-the-art baselines, bridging semantic intelligence and edge reactivity.
          </p>
        </div>

      </div>
    </div>

  </div>
</section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Approach</h2>
          <div class="content has-text-justified has-text-centered">
            <h3 class="title is-4">Motivation</h3>
            <p> VLAs inherit internet-scale knowledge from VLMs, giving them far superior vision and language understanding compared to models trained from scratch. However, scaling up robot policies comes at the cost of inference speed, which can degrade performance. In particular, longer inference times slow the control loop, causing delays that impact overall system behavior. This paper addresses the question: how can large robotic foundation models be deployed on the edge without being limited by computational cost? 
            </p> 
            <p> These challenges are especially pronounced in mobile robots. With a limited ego-centric field of view, robots cannot fully perceive their surroundings from a single observation. Dynamic obstacles, such as pedestrians, exacerbate this issue. Mobile robots must continuously update observations and generate actions based on the latest sensory inputs. Scaling foundation models increases computational load, directly reducing performance in real-world navigation tasks. 
            </p>         
            <h3 class="title is-4">AsyncVLA</h3>
            <p>
            We propose AsyncVLA, an asynchronous navigation system that combines the rich language-visual understanding of robotic foundation models with fast, reactive control. To achieve reactive behavior without losing semantic understanding, AsyncVLA integrates three key components: (1) a lightweight onboard Edge Adapter for action refinement, (2) automatic data re-balancing to encourage responsiveness, and (3) end-to-end training to better align the base VLA with the Edge Adapter. During inference, AsyncVLA separates the system: the base VLA runs on a high-performance workstation, while the Edge Adapter operates on the robot’s controller, processing observations with minimal latency for rapid reactions.
            </p>
            <img src="./static/images/pull.png" style="display:block; margin:auto; width:1000px; height:auto;" />               
            <h3 class="title is-4">Network</h3>
            <p> We build AsyncVLA on OmniVLA, a vision-based robotic foundation model for navigation, which serves as our base VLA. OmniVLA generates actions conditioned on multiple goal modalities, including 2D goal poses, language instructions, and egocentric goal images. It interprets these modalities using large-scale visual encoders (SigLIP and DINOv2) and a language model (LLaMA2 7B). 
            </p> 
            <p> Since the Edge Adapter handles action generation, we feed it OmniVLA’s action token embeddings, which encode both action features and the semantic-visual information interpreted by OmniVLA. These embeddings have a high dimensionality of 8×4×4096 (action chunk size × action dimension × embedding size). Feeding them directly into the Edge Adapter would increase onboard network size and computational cost. To address this, we apply a **token projector** with two MLP ResNet blocks to compress each token from 4×4096 to 1024 dimensions, enabling efficient transmission from the workstation to the robot during inference. 
            </p>
            <img src="./static/images/AsyncVLA.png" style="display:block; margin:auto; width:500px; height:auto;" />                   
          </div>
        </div>
      </div>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <h2 class="title is-3">Experiments</h2>
                <p>
                  We evaluate AsyncVLA on 2D goal pose– and language-conditioned navigation tasks in challenging, cluttered indoor and outdoor environments, including settings with weak WiFi connectivity.
                </p>                                                
          </div>
      </section>     
      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered">

            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <!--/ Interpolating. -->

                <!-- Re-rendering. -->
                <h3 class="title is-4">2D goal pose-conditioned navigation with pedestrians</h3>
                <div class="content has-text-justified">
                  <p>
                  Since our AsyncVLA can react to the latest observation without delay, our AsyncVLA allows the robot to yield to the pedestrian, pass safely without collision, and then proceed toward the goal. In contrast, ``OmniVLA'', which runs inference on a workstation, suffers from delayed action updates and consequently collides with the pedestrian, failing to reach the goal.
                  </p>                
                
                  <video autoplay controls muted loop playsinline width="80%"style="display: block; margin: 0 auto;">
                    <source src="static/videos/compare_ped.mp4" type="video/mp4">
                  </video>                                       
                </div>
          </div>
      </section>

    <!-- Results Carousel -->
    <p>
    We conducted experiments involving various interactions with the robot. The following videos present representative samples from these experiments.
    </p>       
    <section class="hero is-light is-small" style="margin-top: 20px;">      
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-bww1">
                        <video poster="" id="go1_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/ped/nav_ped_1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-tello">
                        <video poster="" id="d435_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/ped/nav_ped_4.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-soda3-left">
                        <video poster="" id="d435_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/ped/nav_ped_5.mp4" type="video/mp4">
                        </video>
                    </div>                       
                    <div class="item item-go1-outside">
                        <video poster="" id="height_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/ped/nav_ped_2.mp4" type="video/mp4">
                        </video>
                    </div>                
                    <div class="item item-soda3-left">
                        <video poster="" id="d435_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/ped/nav_ped_3.mp4" type="video/mp4">
                        </video>
                    </div>                                                      
                </div>
            </div>
        </div>
    </section>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <br />
                <!--/ Interpolating. -->

                <!-- Re-rendering. -->
                <h3 class="title is-4">Robustness for large delay in 2D goal pose-conditioned navigation</h3>
                <div class="content has-text-justified">
                  <p>
                  We evaluate policy robustness under artificially introduced workstation delays of 0.2 s, 2.0 s, and 5.0 s, while the robot’s onboard controller computes actions at its maximum rate. As shown in the following graph, AsyncVLA outperforms OmniVLA even at 0.2 s latency, with the performance gap widening as delays increase. The onboard Edge Adapter adapts the base VLA’s guidance based on current observations, whereas OmniVLA generates actions from stale data, causing navigation performance to degrade under higher latencies.
                  </p>
                  <img src="./static/images/graph_latency.png" style="display:block; margin:auto; width:700px; height:auto;" />   
                  <p>
                  The videos compare AsyncVLA with the strong baseline, OmniVLA. At 5 Hz without delay, both perform well, but at 0.2 Hz with artificial delay, OmniVLA overshoots and fails to navigate. In contrast, AsyncVLA remains reliable, as the onboard Edge Adapter quickly adjusts actions and guides the robot toward the goal.
                  </p>                  
                                       
                  <video autoplay controls muted loop playsinline width="80%" style="display: block; margin: 0 auto;">
                    <source src="static/videos/compare_oclude.mp4" type="video/mp4">
                  </video>                         
                  
                </div>
          </div>
      </section>

    <!-- Results Carousel -->
    <p>
    We conducted quantitative experiments in cluttered environments, with and without pedestrians, navigating the robot to goals up to 30 m away. The following videos show representative samples from experiments without pedestrians.
    </p>           
    <section class="hero is-light is-small" style="margin-top: 20px;">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">                
                    <div class="item item-go1-outside">
                        <video poster="" id="height_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/pose/pose_nav_5.mp4" type="video/mp4">
                        </video>
                    </div>                 
                    <div class="item item-soda3-left">
                        <video poster="" id="d435_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/pose/pose_nav_1.mp4" type="video/mp4">
                        </video>
                    </div>    
                    <div class="item item-soda3-left">
                        <video poster="" id="d435_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/pose/pose_nav_2.mp4" type="video/mp4">
                        </video>
                    </div>          
                    <div class="item item-soda3-left">
                        <video poster="" id="d435_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/pose/pose_nav_3.mp4" type="video/mp4">
                        </video>
                    </div>    
                    <div class="item item-soda3-left">
                        <video poster="" id="d435_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/pose/pose_nav_4.mp4" type="video/mp4">
                        </video>
                    </div>                                                                   
                </div>
            </div>
        </div>
    </section>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <br />
                <!--/ Interpolating. -->

                <!-- Re-rendering. -->
                <h3 class="title is-4">Language-conditioned navigation</h3>
                <div class="content has-text-justified">
                  <p>
                  Since AsyncVLA builds on OmniVLA, it inherits the ability to translate high-level language and visual understanding into actions, enabling effective instruction-following. In our evaluations, AsyncVLA remains robust to out-of-distribution language instructions, achieving performance comparable to OmniVLA.
                  </p>
                </div>
          </div>
      </section>

    <!-- Results Carousel -->
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-bww1">
                        <video poster="" id="bww1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/lan/lan_nav_prompt_1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-tello">
                        <video poster="" id="tello" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/lan/lan_nav_prompt_2.mp4" type="video/mp4">
                        </video>
                    </div>       
                    <div class="item item-tello">
                        <video poster="" id="tello" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/lan/lan_nav_prompt_4.mp4" type="video/mp4">
                        </video>
                    </div>         
                    <div class="item item-tello">
                        <video poster="" id="tello" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/lan/lan_nav_prompt_6.mp4" type="video/mp4">
                        </video>
                    </div>           
                    <div class="item item-tello">
                        <video poster="" id="tello" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/lan/lan_nav_prompt_7.mp4" type="video/mp4">
                        </video>
                    </div>                                                               
                    <div class="item item-tello">
                        <video poster="" id="tello" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/lan/lan_nav_prompt_3.mp4" type="video/mp4">
                        </video>
                    </div>           
                    <div class="item item-tello">
                        <video poster="" id="tello" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/lan/lan_nav_prompt_5.mp4" type="video/mp4">
                        </video>
                    </div>                                                 
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">  
      <div class="container is-max-desktop">    
        <h2 class="title">BibTeX</h2>
        <pre><code> @misc{hirose2026asyncvla,
      title={AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge}, 
      author={Noriaki Hirose and Catherine Glossop and Dhruv Shah and Sergey Levine},
      year={2026},
      eprint={xxxx.xxxxx},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/xxxx.xxxxx}, 
      }</code></pre>            
      </div>
    </section>

    <br>
    <center class="is-size-10">
      The website (<a href="https://github.com/asyncvla/asyncvla.github.io">source code</a>) design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
                class="dnerf">Nerfies</span></a>.
    </center>
    <br>
</body>

</html>
